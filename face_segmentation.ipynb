{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+zkBpQuIcYe3n6qzu9oCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexYiningLiu/UNet_Face_Segmentation/blob/main/face_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtJp86FI-UT9",
        "outputId": "b2fe716e-f2f4-43e6-99c6-e66d39078a9c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro-NGdLk-fVz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import os \n",
        "import zipfile\n",
        "import random\n",
        "import pandas as pd \n",
        "import cv2"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozoP4hmwMb0R"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as K"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoOo19tVBToL"
      },
      "source": [
        "# Data Loading and Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwPclAFXAuQs",
        "outputId": "1fe522bb-e1cd-4e10-fb62-65246cfee97d"
      },
      "source": [
        "df = pd.read_pickle('/content/drive/My Drive/Datasets/annotation.pkl')\n",
        "print(df)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       ImageEyesGazeDirection_0  ...  ImageEyesHeadDirection_1\n",
            "00000                 -0.203216  ...                  0.046461\n",
            "00001                  0.945687  ...                  0.808736\n",
            "00002                 -0.414811  ...                 -0.061110\n",
            "00003                 -0.976813  ...                 -0.301131\n",
            "00004                  0.980972  ...                 -0.182908\n",
            "...                         ...  ...                       ...\n",
            "03995                 -0.650791  ...                  0.576683\n",
            "03996                  0.990405  ...                 -0.763386\n",
            "03997                 -0.745241  ...                 -0.034462\n",
            "03998                 -0.227230  ...                  0.983453\n",
            "03999                 -0.157115  ...                  0.656179\n",
            "\n",
            "[4000 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TXUUDPR-jEU"
      },
      "source": [
        "raw_dir = '/content/drive/My Drive/Datasets/Unzipped/output/lit'\n",
        "seg_dir = '/content/drive/My Drive/Datasets/Unzipped/output/segmentation'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW0dwioCAREF",
        "outputId": "95e93806-4223-4f2e-e967-771c44ddddfb"
      },
      "source": [
        "raw_images = np.load('/content/drive/My Drive/Datasets/raw_imgs.npy')\n",
        "print(raw_images.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKR8QLm_A2eK"
      },
      "source": [
        "**Produce np file from segmentation masks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FexqqwnFHvko"
      },
      "source": [
        "Only use the face color mask. Ignore the other masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNFgtm2GAdQy"
      },
      "source": [
        "images = []\n",
        "face_color = (205, 158, 0)\n",
        "for index, i in enumerate(df.index.values):\n",
        "    image_path = os.path.join(seg_dir, i+'.png')\n",
        "    image = cv2.imread(image_path)\n",
        "    image = image[:, 80:560]\n",
        "    image = cv2.resize(image, (256, 256))\n",
        "    mask = np.zeros(image.shape[:2], dtype=bool)\n",
        "    mask |= (image == face_color).all(-1)\n",
        "    image[mask] = (255, 255, 255)\n",
        "    image[~mask] = (0,0,0)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    images.append(image)\n",
        "    if index%100==0:\n",
        "        print(\"Image number: %d\" %index)\n",
        "images = np.array(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVfgdr7HBDb7"
      },
      "source": [
        "np.save('/content/drive/My Drive/Datasets/face_masks.npy', images)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kruEpuoefvD",
        "outputId": "7f0641db-e527-421c-c32b-cd6c68467153"
      },
      "source": [
        "segmentation_images = np.load('/content/drive/My Drive/Datasets/segmentation_imgs.npy')\n",
        "print(segmentation_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJyCO8SUK8YS",
        "outputId": "2c7e92dc-e52b-4c2b-ec24-6e55127a6c8a"
      },
      "source": [
        "face_mask_images = np.load('/content/drive/My Drive/Datasets/face_masks.npy')\n",
        "print(face_mask_images.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 256, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-t5sMwLN-Cy"
      },
      "source": [
        "np.random.seed(100)\n",
        "shuffle_ids = np.array([i for i in range(4000)])\n",
        "np.random.shuffle(shuffle_ids)\n",
        "train_ids = shuffle_ids[:int(4000*0.8)]\n",
        "val_ids = shuffle_ids[int(4000*0.8):int(4000*0.8+400)]\n",
        "test_ids = shuffle_ids[int(4000*0.8+400):]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52M1ZxGHOWLq"
      },
      "source": [
        "train_images, train_masks = raw_images[train_ids], face_mask_images[train_ids]\n",
        "val_images, val_masks = raw_images[val_ids], face_mask_images[val_ids]\n",
        "test_images, test_masks = raw_images[test_ids], face_mask_images[test_ids]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trBNdl_fOrtZ",
        "outputId": "c0d05dfd-d2c8-49b8-b386-27b7002c0bda"
      },
      "source": [
        "print(train_images.shape, val_images.shape, test_images.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3200, 256, 256, 3) (400, 256, 256, 3) (400, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeRoq_nmc7VB"
      },
      "source": [
        "train_masks = train_masks / train_masks.max()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25y6jcTgdBFp"
      },
      "source": [
        "val_masks = val_masks / val_masks.max()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eBzeNt_UHLb"
      },
      "source": [
        "del raw_images\n",
        "del face_mask_images"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUXSAT5Pge31"
      },
      "source": [
        "# Hyperparameter Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dosTruG8giDU"
      },
      "source": [
        "BATCH_SIZE = 16 \n",
        "EPOCHS = 60\n",
        "# Patience for the learning rate\n",
        "LR_PATIENCE = 5\n",
        "\n",
        "# Patience for early stopping\n",
        "STOPPING_PATIENCE = 10\n",
        "\n",
        "SHAPE = (256, 256, 3)\n",
        "\n",
        "# Number of classes\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "CLASSES = {0:'Background', 1:'Face'}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU69TQZ2BZXD"
      },
      "source": [
        "# Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUeKdemmJYvZ"
      },
      "source": [
        "def getModel():\n",
        "    \n",
        "    # Use the TPU strategy\n",
        "    #with tpu_strategy.scope():\n",
        "        \n",
        "    # Build the model\n",
        "    input_img = Input((SHAPE[0], SHAPE[1], 3), name='img')\n",
        "\n",
        "    c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (input_img)\n",
        "    c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
        "    c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
        "    c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
        "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
        "\n",
        "    u5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c4)\n",
        "    u5 = concatenate([u5, c3])\n",
        "    c6 = Conv2D(32, (3, 3), activation='relu', padding='same') (u5)\n",
        "    c6 = Conv2D(32, (3, 3), activation='relu', padding='same') (c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c2])\n",
        "    c7 = Conv2D(16, (3, 3), activation='relu', padding='same') (u7)\n",
        "    c7 = Conv2D(16, (3, 3), activation='relu', padding='same') (c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c1])\n",
        "    c8 = Conv2D(8, (3, 3), activation='relu', padding='same') (u8)\n",
        "    c8 = Conv2D(8, (3, 3), activation='relu', padding='same') (c8)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c8)\n",
        "\n",
        "    model = Model(inputs=[input_img], \n",
        "                    outputs=[outputs])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    \n",
        "    model.compile(optimizer=optimizer, \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_54yFmrQH3h",
        "outputId": "fc42015b-4a4c-4ca2-f5e7-94ef3c8f9a14"
      },
      "source": [
        "model = getModel()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "img (InputLayer)                [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 256, 256, 8)  224         img[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 256, 256, 8)  584         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 128, 128, 8)  0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 128, 128, 16) 1168        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 128, 128, 16) 2320        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 16)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 64, 64, 32)   4640        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 64, 64, 32)   9248        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 32)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 32, 32, 64)   18496       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 32, 32, 64)   36928       conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 64, 64, 64)   16448       conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 64, 64, 96)   0           conv2d_transpose_3[0][0]         \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 64, 64, 32)   27680       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 64, 64, 32)   9248        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 128, 128, 32) 4128        conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 128, 128, 48) 0           conv2d_transpose_4[0][0]         \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 128, 128, 16) 6928        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 128, 128, 16) 2320        conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 256, 256, 16) 1040        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 256, 256, 24) 0           conv2d_transpose_5[0][0]         \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 256, 256, 8)  1736        concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 256, 256, 8)  584         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 256, 256, 1)  9           conv2d_28[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 143,729\n",
            "Trainable params: 143,729\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBdAEmIYSiwp"
      },
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(patience=STOPPING_PATIENCE, verbose=1),\n",
        "    ReduceLROnPlateau(patience=LR_PATIENCE, verbose=1),\n",
        "    ModelCheckpoint('/content/drive/My Drive/Datasets/model_face_seg_v1.h5', verbose=1, save_best_only=True)\n",
        "]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bZArAr0Pe6K",
        "outputId": "88c7383b-2034-4933-c100-b81dd03912b4"
      },
      "source": [
        "results = model.fit(train_images, train_masks, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks,\n",
        "                    validation_data=(val_images, val_masks))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0446 - val_accuracy: 0.9804\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.04460, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 2/60\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0406 - accuracy: 0.9824 - val_loss: 0.0346 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.04460 to 0.03461, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 3/60\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0331 - accuracy: 0.9859 - val_loss: 0.0301 - val_accuracy: 0.9872\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03461 to 0.03012, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 4/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0292 - accuracy: 0.9877 - val_loss: 0.0264 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.03012 to 0.02644, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 5/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0257 - accuracy: 0.9892 - val_loss: 0.0255 - val_accuracy: 0.9893\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02644 to 0.02548, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 6/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0240 - accuracy: 0.9900 - val_loss: 0.0231 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.02548 to 0.02312, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 7/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0225 - accuracy: 0.9906 - val_loss: 0.0219 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.02312 to 0.02193, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 8/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0218 - accuracy: 0.9909 - val_loss: 0.0210 - val_accuracy: 0.9912\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.02193 to 0.02099, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 9/60\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0206 - accuracy: 0.9914 - val_loss: 0.0207 - val_accuracy: 0.9912\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.02099 to 0.02073, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 10/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 0.0196 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.02073 to 0.01963, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 11/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0190 - accuracy: 0.9921 - val_loss: 0.0189 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01963 to 0.01890, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 12/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.0184 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01890 to 0.01838, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 13/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.0178 - val_accuracy: 0.9926\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01838 to 0.01775, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 14/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0174 - accuracy: 0.9927 - val_loss: 0.0176 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01775 to 0.01760, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 15/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0170 - accuracy: 0.9929 - val_loss: 0.0177 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01760\n",
            "Epoch 16/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0168 - accuracy: 0.9930 - val_loss: 0.0175 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.01760 to 0.01748, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 17/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0165 - accuracy: 0.9931 - val_loss: 0.0172 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01748 to 0.01718, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 18/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0162 - accuracy: 0.9932 - val_loss: 0.0164 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01718 to 0.01643, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 19/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0157 - accuracy: 0.9934 - val_loss: 0.0167 - val_accuracy: 0.9931\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01643\n",
            "Epoch 20/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0156 - accuracy: 0.9935 - val_loss: 0.0160 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01643 to 0.01596, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 21/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0153 - accuracy: 0.9936 - val_loss: 0.0167 - val_accuracy: 0.9931\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01596\n",
            "Epoch 22/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0153 - accuracy: 0.9936 - val_loss: 0.0154 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.01596 to 0.01541, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 23/60\n",
            "200/200 [==============================] - 16s 82ms/step - loss: 0.0148 - accuracy: 0.9938 - val_loss: 0.0160 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01541\n",
            "Epoch 24/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0147 - accuracy: 0.9938 - val_loss: 0.0163 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01541\n",
            "Epoch 25/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0149 - accuracy: 0.9938 - val_loss: 0.0151 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.01541 to 0.01508, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 26/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0143 - accuracy: 0.9940 - val_loss: 0.0155 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01508\n",
            "Epoch 27/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0143 - accuracy: 0.9940 - val_loss: 0.0153 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01508\n",
            "Epoch 28/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0158 - accuracy: 0.9934 - val_loss: 0.0154 - val_accuracy: 0.9936\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01508\n",
            "Epoch 29/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0141 - accuracy: 0.9941 - val_loss: 0.0146 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.01508 to 0.01464, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 30/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0138 - accuracy: 0.9942 - val_loss: 0.0149 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01464\n",
            "Epoch 31/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0136 - accuracy: 0.9943 - val_loss: 0.0148 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01464\n",
            "Epoch 32/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0134 - accuracy: 0.9943 - val_loss: 0.0145 - val_accuracy: 0.9940\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.01464 to 0.01448, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 33/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0135 - accuracy: 0.9943 - val_loss: 0.0150 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01448\n",
            "Epoch 34/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0132 - accuracy: 0.9944 - val_loss: 0.0145 - val_accuracy: 0.9940\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01448\n",
            "Epoch 35/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0130 - accuracy: 0.9945 - val_loss: 0.0148 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01448\n",
            "Epoch 36/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0130 - accuracy: 0.9945 - val_loss: 0.0147 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01448\n",
            "Epoch 37/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0128 - accuracy: 0.9946 - val_loss: 0.0148 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01448\n",
            "Epoch 38/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0120 - accuracy: 0.9949 - val_loss: 0.0140 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.01448 to 0.01402, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 39/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0118 - accuracy: 0.9950 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.01402 to 0.01400, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 40/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0117 - accuracy: 0.9950 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01400\n",
            "Epoch 41/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0117 - accuracy: 0.9950 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.01400 to 0.01398, saving model to /content/drive/My Drive/Datasets/model_face_seg_v1.h5\n",
            "Epoch 42/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0117 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01398\n",
            "Epoch 43/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0116 - accuracy: 0.9951 - val_loss: 0.0141 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01398\n",
            "Epoch 44/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0115 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01398\n",
            "Epoch 45/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0115 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01398\n",
            "Epoch 46/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0115 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01398\n",
            "Epoch 47/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0115 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01398\n",
            "Epoch 48/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0115 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01398\n",
            "Epoch 49/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0115 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01398\n",
            "Epoch 50/60\n",
            "200/200 [==============================] - 16s 81ms/step - loss: 0.0115 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01398\n",
            "Epoch 51/60\n",
            "200/200 [==============================] - 17s 84ms/step - loss: 0.0115 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9943\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01398\n",
            "Epoch 00051: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "QoTSWpQngO2I",
        "outputId": "822b9bf6-3c6b-4a43-c9f1-b6c2281d5d6b"
      },
      "source": [
        "NUMBER = 19\n",
        "\n",
        "my_preds = model.predict(np.expand_dims(test_images[NUMBER], 0))\n",
        "my_preds = my_preds.flatten()\n",
        "my_preds = np.array([1 if i >= 0.5 else 0 for i in my_preds])\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
        "ax[0].imshow(my_preds.reshape(256, 256))\n",
        "ax[0].set_title('Prediction')\n",
        "ax[1].imshow(test_masks[NUMBER].reshape(256, 256))\n",
        "ax[1].set_title('Ground truth')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Ground truth')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}